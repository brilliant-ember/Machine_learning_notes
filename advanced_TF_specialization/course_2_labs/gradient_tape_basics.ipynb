{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient tap automatic differentiation \n",
    "\n",
    "https://github.com/sasidhar-programmer/Tensorflow_Advance_Techniques/blob/main/2-custom_and_distributed_training/week-1/C2_W1_Lab_2_gradient-tape-basics.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "x = tf.Tensor(\n[[1. 1.]\n [1. 1.]], shape=(2, 2), dtype=float32)\n\ny = tf.Tensor(4.0, shape=(), dtype=float32)\n\nz = tf.Tensor(16.0, shape=(), dtype=float32)\n\ndz/dx =  None\n"
    }
   ],
   "source": [
    "x = tf.ones([2,2])\n",
    "print(\"x =\",x)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x) # record actions performed on tensor x as it is 'watched' if we dont do this the result of taking the gradient with respect to x will be None\n",
    "    y = tf.reduce_sum(x) # y = 4\n",
    "    print(\"\\ny =\", y)\n",
    "    z = tf.square(y) # z = y^2 = 16\n",
    "    print(\"\\nz =\", z)\n",
    "\n",
    "dz_dx = tape.gradient(z,x) # derive z with respect to x\n",
    "# refer to course2_how_it_works_and_distributed.md to understand how the gradient was obtained\n",
    "print(\"\\ndz/dx = \", dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(108.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "# the tape is good for only one derivation, if you want to use more than one then\n",
    "# set peristannt =true\n",
    "x= tf.constant(3.0)\n",
    "with tf.GradientTape() as t:\n",
    "    t.watch(x)\n",
    "    y = x * x # y=x^2\n",
    "    z = y*y # z=y^2\n",
    "\n",
    "#dz/dx = 4 * x^3 = 108\n",
    "dz_dx = t.gradient(z,x)\n",
    "print(dz_dx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "GradientTape.gradient can only be called once on non-persistent tapes.\n"
    }
   ],
   "source": [
    "# now we get an error if we try to use the gradient tape t again since it is not persistant\n",
    "try:\n",
    "    dy_dx = t.gradient(y,x) # dy/dx = 2x = 6\n",
    "    print(dy_dx)\n",
    "except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(108.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "# we set it to persistant now and can perform the gradient with the tape as many times as we want\n",
    "\n",
    "# but we will need to manually delete the tape after we're done with it\n",
    "with tf.GradientTape(persistent=True) as t:\n",
    "    t.watch(x)\n",
    "    y = x * x # y=x^2\n",
    "    z = y*y # z=y^2\n",
    "\n",
    "#dz/dx = 4 * x^3 = 108\n",
    "dz_dx = t.gradient(z,x)\n",
    "print(dz_dx)\n",
    "\n",
    "dy_dx = t.gradient(y,x) # dy/dx = 2x = 6\n",
    "print(dy_dx)\n",
    "\n",
    "del t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(3.0, shape=(), dtype=float32)\ntf.Tensor(6.0, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "# higher order derivatives\n",
    "# the first grad should be computed inside one of the with blocks\n",
    "# but the second grad should be indented as much as the first grad or less\n",
    "# ie it should be either in an outer block or in the same block of grad1\n",
    "x = tf.Variable(1.0)\n",
    "with tf.GradientTape() as tape2:\n",
    "    with tf.GradientTape()as tape1:\n",
    "        y= x*x*x\n",
    "    dy_dx = tape1.gradient(y,x)\n",
    "\n",
    "\n",
    "d2y_dx2 = tape2.gradient(dy_dx,x)\n",
    "print(dy_dx)\n",
    "print(d2y_dx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}